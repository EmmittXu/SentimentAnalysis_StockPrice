{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "data loaded!\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print \"loading data...\"\n",
    "x = cPickle.load(open(\"Data/imdb-train-val-test.pickle\", \"rb\"))\n",
    "revs, word2vec, word_idx_map = x[0], x[1], x[2]\n",
    "print \"data loaded!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150855, 300)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def sentence2_1hot(sentence, max_len, word_idx_map):\n",
    "    V=np.zeros(shape=max_len, dtype=np.int32)\n",
    "    i=0\n",
    "    for word in sentence.split():\n",
    "        V[i]=word_idx_map[word]\n",
    "        i+=1\n",
    "        if i>=35:\n",
    "            break\n",
    "    return V\n",
    "max_sentence_len=np.max((pd.DataFrame(revs)['num_words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32081, 17842, 28011,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp=sentence2_1hot(\"my name is\", 35, word_idx_map)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_batch(revs, W, max_sentence_len=35, batch_size=20, batch_index=1, embedding_size=300):\n",
    "    tail=batch_size*(batch_index+1)\n",
    "    assert tail<=len(revs)\n",
    "    embedded=np.zeros(shape=(batch_size, max_sentence_len, embedding_size, 1), dtype=np.float32)\n",
    "    labels=np.zeros(shape=batch_size, dtype=np.int32)\n",
    "    k=0\n",
    "    for i in range(tail-batch_size, tail):\n",
    "        sentence_1hot=sentence2_1hot(revs[i]['text'], max_sentence_len, word_idx_map)\n",
    "        for j in range(len(sentence_1hot)):\n",
    "            embedded[k][j][:][:]=W[sentence_1hot[j]][:embedding_size].reshape(embedding_size,1)\n",
    "        labels[k]=int(revs[i]['y']*0.25)\n",
    "        k+=1\n",
    "    return embedded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size=50\n",
    "X_train_num, y_train = embedding_batch(revs, word2vec, batch_size=1500, batch_index=0, embedding_size=embedding_size)\n",
    "X_valid_num, y_valid = embedding_batch(revs, word2vec, batch_size=200, batch_index=20, embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.508666666667\n"
     ]
    }
   ],
   "source": [
    "m,n=0,0\n",
    "\n",
    "for i in y_train:\n",
    "    if i==0:\n",
    "        m+=1.0\n",
    "    if i==1:\n",
    "        n+=1.0\n",
    "print(m/(m+n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525\n"
     ]
    }
   ],
   "source": [
    "m,n=0,0\n",
    "\n",
    "for i in y_valid:\n",
    "    if i==0:\n",
    "        m+=1.0\n",
    "    if i==1:\n",
    "        n+=1.0\n",
    "print(m/(m+n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None, 35, embedding_size,1], name=\"X_input\")\n",
    "y=tf.placeholder(tf.int64, shape=None, name=\"y_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Embedding layer, embedding size set to 300\n",
    "# embedding_size=100\n",
    "# with tf.name_scope(\"embedding_layer\"):\n",
    "#     W=tf.Variable(tf.random_uniform([len(vocabulary), embedding_size], -1.0,1.0), name=\"W\")\n",
    "#     #W=tf.Variable(tf.random_uniform([5000, embedding_size], -1.0,1.0), name=\"W\")\n",
    "#     embedded_chars=tf.nn.embedding_lookup(W, X)\n",
    "#     embedded_chars_expanded=tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "\n",
    "#embedded_chars_expanded=tf.expand_dims(X,-1)\n",
    "#Convolution layer\n",
    "#embedded_chars_expanded=X\n",
    "\n",
    "# y_train=np.array(y_train, dtype=np.int32)\n",
    "# y_valid=np.array(y_valid, dtype=np.int32)\n",
    "pooled_outputs=[]\n",
    "n_filters=10\n",
    "\n",
    "with tf.name_scope(\"conv_layer\"):\n",
    "    filter_shape=[3, embedding_size, 1, n_filters]\n",
    "    W=tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"Weights\")\n",
    "    b=tf.Variable(tf.constant(0.1, shape=[n_filters]), name=\"bias\")\n",
    "    conv=tf.nn.conv2d(X,\n",
    "                         W,\n",
    "                         strides=[1,1,1,1],\n",
    "                         padding='SAME',\n",
    "                          name=\"conv\")\n",
    "    h=tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "    \n",
    "    pooled=tf.nn.max_pool(h,\n",
    "                             ksize=[1, 1, embedding_size, 1],\n",
    "                              strides=[1,1,1,1],\n",
    "                             padding='VALID',\n",
    "                             name=\"pooling\")\n",
    "    #pooled_flat=tf.contrib.layers.flatten(pooled)\n",
    "\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop=tf.nn.dropout(pooled, 0.5)\n",
    "    h_drop_flattened=tf.contrib.layers.flatten(h_drop)\n",
    "    \n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.Variable(tf.truncated_normal([35*n_filters, 2], stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[2]), name=\"b\")\n",
    "    scores = tf.nn.xw_plus_b(h_drop_flattened, W, b, name=\"scores\")\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "    \n",
    "# Calculate mean cross-entropy loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    #losses = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "    xentropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "        \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer=tf.train.AdamOptimizer(0.01)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "\n",
    "# Calculate Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "n_epochs=500\n",
    "batch_size=5\n",
    "best_validation_loss=np.infty\n",
    "max_checks_no_progress=20\n",
    "check_without_process=0\n",
    "n_batch=len(revs)//batch_size\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    start=time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(n_batch):\n",
    "            X_batch, y_batch=embedding_batch(revs, W=word2vec, max_sentence_len=35, \\\n",
    "                                        batch_size=batch_size, batch_index=batch, embedding_size=embedding_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        training_acu=accuracy.eval(feed_dict={X:X_train_num, y:y_train})\n",
    "        validation_acu=accuracy.eval(feed_dict={X:X_valid_num, y:y_valid})\n",
    "        print(\"Epoch:\", epoch, \"training:\", training_acu, \"validation:\", validation_acu)\n",
    "        validation_loss=loss.eval(feed_dict={X:X_valid_num, y:y_valid})\n",
    "        if(validation_loss<best_validation_loss):\n",
    "            #save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "            best_validation_loss=validation_loss\n",
    "            check_without_process=0\n",
    "        else:\n",
    "            check_without_process+=1\n",
    "            if check_without_process>=max_checks_no_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break;\n",
    "print(\"Training completed in \", time.time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
